{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Artificial Neural Network - Prediction of Sleep and Awake States Including Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries and packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "# To ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RSEED=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the files\n",
    "data_directory = '../data/file_per_night'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already run the following cells and have the files 'train_data.parquet' and 'test_data.parque' in the 'data/file_per_night' directory, you can skip the following cells and continue with importing those two files into new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all files in the directory\n",
    "file_paths = [os.path.join(data_directory, file) for file in os.listdir(data_directory) if os.path.isfile(os.path.join(data_directory, file))]\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "dataframes = [pd.read_parquet(file) for file in file_paths]\n",
    "full_dataframe = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have only two classes, we set the event 'onset' to 'awake' and 'wakeup' to 'sleep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataframe['event'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in full_dataframe.index:\n",
    "    if full_dataframe['event'][i] == 'onset':\n",
    "        full_dataframe['event'][i] = 'awake'\n",
    "    elif full_dataframe['event'][i] == 'wakeup':\n",
    "        full_dataframe['event'][i] = 'sleep'\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataframe.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Nas\n",
    "full_dataframe.drop(['night', 'anglez_enmo_ratio'], axis = 1, inplace =True)\n",
    "full_dataframe.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataframe.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform the regular train-test-split for training and evaluation of the model as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets with stratification\n",
    "train_data, test_data = train_test_split(full_dataframe, test_size=0.25, stratify=full_dataframe['event'], random_state=RSEED)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safe the train and test sets for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_parquet(os.path.join(data_directory, \"train_data.parquet\"))\n",
    "test_data.to_parquet(os.path.join(data_directory, \"test_data.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> continue here, if you already created 'train_data.parque' and 'test_data.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to read the train and test set, if they were already created\n",
    "train_data = pd.read_parquet(os.path.join(data_directory, \"train_data.parquet\"))\n",
    "test_data = pd.read_parquet(os.path.join(data_directory, \"test_data.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we select all the newly engineered features that we created including several statistical values for the corresponding bins such as mean, standard deviation and maximum, but also so values associated with previous timepoints. For further information regarding feature engineering refer to [this file]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variable from the data\n",
    "X_train = train_data.drop(['event', 'series_id', 'step'], axis=1).reset_index(drop=True)\n",
    "y_train = train_data['event'].reset_index(drop=True)\n",
    "\n",
    "X_test = test_data.drop(['event', \"series_id\", \"step\"], axis=1).reset_index(drop=True)\n",
    "y_test = test_data['event'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For oversampling (because we have more sleep than awake states), we apply SMOTE\n",
    "smote = SMOTE(random_state=RSEED)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "X_train= X_train_smote\n",
    "y_train = y_train_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to reuse the 'smoted' data sets again (also for other models), you could now save it and afterwards only need to reload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe the oversampled train data into PARQUET and CSV files respectively\n",
    "X_train.to_parquet(os.path.join(data_directory, \"X_train_smote.parquet\"))\n",
    "y_train.to_csv(os.path.join(data_directory, \"y_train_smote.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to reload the oversampled train data\n",
    "X_train = pd.read_parquet(os.path.join(data_directory, \"X_train_smote.parquet\"))\n",
    "y_train = pd.read_csv(os.path.join(data_directory, \"y_train.csv\"))['event']\n",
    "\n",
    "\n",
    "X_test = test_data.drop(['event', \"series_id\", \"step\"], axis=1).reset_index(drop=True)\n",
    "y_test = test_data['event'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model type : Classic Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the ANN model architecture with 2 hidden layers and 1 output layer (binary classification)\n",
    "def create_model(N_features):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(N_features,)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of features for the model \n",
    "N_features = 31  \n",
    "model = create_model(N_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose file_path for of the train and test data\n",
    "file_path = '/data/file_per_night/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test data\n",
    "y_train = pd.read_csv(os.path.join(file_path, 'y_train_smote.csv'))['event']\n",
    "X_train = pd.read_parquet(os.path.join(file_path, 'train_data_smote.parquet'))\n",
    "X_test = pd.read_parquet(os.path.join(file_path, 'test_data.parquet')).drop(['event', \"series_id\", \"step\"], axis=1).reset_index(drop=True)\n",
    "y_test = pd.read_parquet(os.path.join(file_path, 'test_data.parquet'))['event'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable for train and test\n",
    "y_train = LabelEncoder().fit_transform(y_train) #sleep = 1, awake = 0\n",
    "y_test = LabelEncoder().fit_transform(y_test) #sleep = 1, awake = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and evaluate it on the test set\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=64, validation_split=0.2 )\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained ANN model in an h5 file\n",
    "model.save('model/ANN_trained_on_full_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained ANN model\n",
    "model = load_model('model/ANN_trained_on_full_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable for the test set\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the predicted probabilities to binary values and save them in a list \n",
    "event = [] \n",
    "for i in y_pred:\n",
    "    if i.round(0) == 0.0:\n",
    "        event.append('awake')\n",
    "    else:\n",
    "        event.append('sleep')\n",
    "y_pred = event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy on test data: {accuracy:.2%}\")\n",
    "print(f\"Precision on test data: {precision:.2%}\")\n",
    "print(f\"Recall on test data: {recall:.2%}\")\n",
    "print(f\"F1 Score on test data: {f1:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.xticks(ticks = [0.5,1.5], labels= ['awake', 'sleep'])\n",
    "plt.ylabel('Actual')\n",
    "plt.yticks(ticks = [0.5,1.5], labels= ['awake', 'sleep'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
